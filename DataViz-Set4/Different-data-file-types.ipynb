{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50cf8dd8",
   "metadata": {},
   "source": [
    "# Different types of data files\n",
    "\n",
    "We have already seen a few different types of data. Here we will discuss a few other types of file containers that you may come across, and how these can be brought into a form for use in visualisation.\n",
    "\n",
    "# Structured data\n",
    "\n",
    "Typically we have been using pandas for getting structured data into python, we'll continue that pattern here and give some examples of other sorts of structured data that can be read using pandas first.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "## CSV files\n",
    "\n",
    "Csv files are one of the simplest structured data formats, essentially encoding a single page of data in spreadsheet-like form consisting of rows and columns. The easiest way to read such files is using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('example.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a77dc",
   "metadata": {},
   "source": [
    "- .csv stands for comma separated values, since the file is structured as a text file where the entries along each row are separated by a comma and then each row starts on a new line. \n",
    "- Because it is quite simple there are not really many different ways to read in the file differently using pandas. - One main thing would be the separator `sep`  option that can be used, relatively commonly other characters are used to separate values rather than commas (the tab character is one that is sometimes used resulting in a tsv tab separated values file). \n",
    "- Adding the `sep` option allows control over reading in a file where a different character is used eg. `pd.read_csv('filename.csv',sep=':')`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90bff2",
   "metadata": {},
   "source": [
    "## Excel files\n",
    "\n",
    "Excel files allow storage of more formatting and embedded calculation than a csv file, but essentially store the same sort of structured data consisting of rows/columns of data. Again, we have already used this sort of file in previous examples. There are various options that can be used when reading in [excel files](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html#pandas.read_excel), allowing skipping of rows, and reading in particular columns of data. \n",
    "\n",
    "However it is often just simpler to copy the data out of a more complicated spreadsheet and isolate it in a simpler single spreadsheet before reading it into python. \n",
    "\n",
    "- If that isn't an option (usually if you are ingesting lots of spreadhseets with the same format say), then it can be worth spending the time to tune the `read_excel()` options to skip rows or change the value for missing data. - - One thing it might be worth giving an example for is reading in different sheets from a single `.xlsx` file, as sometimes data comes across multiple sheets of the same file, here is an example of that using the `sheet_name` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4062187",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('example.xlsx',sheet_name=0)\n",
    "print(data.head())\n",
    "\n",
    "data2 = pd.read_excel('example.xlsx',sheet_name=1)\n",
    "print(data2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2cc7c",
   "metadata": {},
   "source": [
    "## Json files\n",
    "\n",
    "Json files again store structured data in a different format. \n",
    "\n",
    "- They are more flexible than the formats above and can store a more general database entry with data of various types, and with sub-entries. \n",
    "- They again structure the data in a text file, using curly braces {} to indicate different fields of data, and then a colon structure to indicate where a value is given. \n",
    "- If you look at a json file in a text editor you can get a good sense of the structure it contains - generally they tend to hold data more typically in long rather than wide format https://altair-viz.github.io/user_guide/data.html#long-form-vs-wide-form-data. \n",
    "- Again pandas is generally good at reading in most json files, and trying to get them into a useable format for python internally.\n",
    "\n",
    "Here is a quick example of reading in a json with a simple spreadsheet-style of data within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9add02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('example.json')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157394a8",
   "metadata": {},
   "source": [
    "## Stata, Sas and SPSS (For later usage maybe)\n",
    "\n",
    "Pandas is also able to read in data files saved out directly from various statistical software. Some of these are used for various elements of statistical analysis and are quite common particularly across social sciences. Files saved out directly from all of these pieces of sofware have particular file formats, and pandas can read these into a dataframe https://pandas.pydata.org/docs/reference/io.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbe60b",
   "metadata": {},
   "source": [
    "## SQL (For later usage maybe)\n",
    "\n",
    "More complicated and extensive data is often held in a database, which is a more formal set of structures and rules for storing and querying larger amounts of data. The most commonly used database format is probably sql (structured query langauge), interacting with such databases is very well supported in python, with a range of modules supporting different variants of the sql format https://realpython.com/python-sql-libraries/. \n",
    "\n",
    "Dealing in detail with such data is really beyond the scope of the course, but the following is a very basic example of retrieving a simple dataframe from an sql database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e522cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///example.db', echo=False)\n",
    "\n",
    "data = pd.read_sql_table('example',con=engine)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0741c92",
   "metadata": {},
   "source": [
    "## hdf5 using module h5py (For later usage maybe)\n",
    "\n",
    "Hdf5 files are another relatively complex file type which is capable of storing large amounts of structured data. They are commonly used for the transfer of large scientific datasets (which often consist of data values on a regular spatial or temporal grid), along with metadata. Again python has a good dedicated support for such files using a dedicated module h5py https://docs.h5py.org/en/stable/quick.html#quick\n",
    "\n",
    "Here is a quick example of reading in an array of data using h5py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53861e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('example.hdf5', 'r') as f:\n",
    "    print(f.keys())\n",
    "    data = f['a']\n",
    "    print(data)\n",
    "    print(data[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9a4fe",
   "metadata": {},
   "source": [
    "# Unstructured data\n",
    "\n",
    "## Text files\n",
    "\n",
    "- A plain text file can contain an extended piece of writing as text. \n",
    "- Without any real imposition of structure when the file is created, this is inherently unstructured, and therefore not well suited to be read into a dataframe using pandas. \n",
    "- Instead we would usually read a text file directly and keep each line of the file in a list of strings, with each string being the text contained in that line.\n",
    "\n",
    "Here is an example of that using the function `readlines()` which is available in python by default without a module to read the contents of a text file line by line and put them in a list, here that list is contained in the variable `lines`. We can refer to the text within the variable line by line by extracting a particular line number (recall numbering starts from 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "print(lines) \n",
    "print()\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b6cbc",
   "metadata": {},
   "source": [
    "To actually meaningfully do something with the text from the file we need to do further analysis. We talk about this a little more in the notebook dedicated to text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d3c8ec",
   "metadata": {},
   "source": [
    "## html using module Beautiful soup\n",
    "\n",
    "Lastly, a semi-structured form of data is contained in regular webpages in the html (hypertext markup language) files that make them up. \n",
    "- There are elements of structure such as titles and body text, that can be separated and analysed, but most pages also have a large unstructured body text that makes up the main text content on the page that is itself unstructured in the same manner as a plain text file. \n",
    "\n",
    "There is again a dedicated python module to load and interact with html files called Beautiful Soup https://beautiful-soup-4.readthedocs.io/en/latest/#making-the-soup. Together with the urllib module which can make requests for data across the internet and grab the result, we have the very basics of a 'web scraper' capable of requesting a webpage and saving the source of the page as text for later analysis.\n",
    "\n",
    "Below is an example of making a query to Wikipedia for the article about Edinburgh, and then extracting parts of the data on that page using the Beautiful Soup module to access the tagged parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49596d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "http = urllib3.PoolManager()\n",
    "page = http.request('GET', 'https://en.wikipedia.org/wiki/Edinburgh')\n",
    "soup = BeautifulSoup(page.data, 'html.parser')\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.body)\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc884ccd",
   "metadata": {},
   "source": [
    "**WARNING:** Web scraping is something which many websites now discourage in their terms of service (in the early days of the web scrapers and spiders used for accessing the web in an automated way were more common). Some sites (like wikipedia) offer an api instead to get access to their data. Sites sometimes ban access to their web pages to computers which make too many or too frequent access using an automated web scraping tool. So if you intend to make use of the techniques below try to be a good citizen and not abuse the use of web scrapers, and keep in mind the discouragement many sites will make of this in the TOS before scraping their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da2df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
